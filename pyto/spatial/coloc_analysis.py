"""
Generation, preprocessing and analysis of colocalization results

Attributes that hold colocalization data are of the form:
  - 3-colocalization, all synapses together: pre_tether_post_data
  - 3-colocalization, individual synapses: pre_tether_post_data_syn

Other attributes:

Methods to read and preprocess data:

Superseed colocalization.py module.
 
# Author: Vladan Lucic (Max Planck Institute for Biochemistry)
# $Id$
"""

__version__ = "$Revision$"

import os
import sys
import subprocess
import pickle
import itertools
import time
import re
import functools

import numpy as np
import scipy as sp
import pandas as pd 
import matplotlib as mpl
import matplotlib.pyplot as plt

import pyto
from pyto.io.pandas_io import PandasIO
from . import coloc_functions as col_func
from .coloc_table_read import ColocTableRead
from .coloc_pyseg import ColocPyseg


class ColocAnalysis(ColocTableRead, ColocPyseg):
    """
    Provides class methods to generate raw colocalization data by running
    an external PySeg colocalization script and preprocess raw colocalization
    data. The resulting colocalization data are saved in an object of this 
    class.

    An instance of this class contains colocalization data for one
    colocalization project.

    A colocalization project comprises multiple colocalizations, where each 
    colocalization is defined by a distance and a colocalization name
    (comprises names of particle sets used for the colocalization).
   
    A single Colocalization is obtained by colocalizing 2 or more particle 
    sets. The data for each coloclalization comprizes two (pandas) 
    data tables, saved as the following attributes:
      - name_data: combined data for all tomograms, one row for each distance
      - name_data_syn: data for individual tomograms, one row for each
      combination of tomogram and colocalization distance
    where name is a colocalization name.

    Colocalization names are formed by concatenating particle set names with
    '_' placed between the names, such as (pre_post_tether or setX_setY).
    Particle set names should not contain  '_' and should be of the form 
    so that they can be used as python variables (so no '-', ...).

    Note that colocalize_run() requires coloclization names with the number
    of particle sets prepended (such as 3_pre_post_tether) or 2_setX_setY).
    
    Table columns (of both individual and combined tomos tables) obtained 
    directly from the coloclization script run:
      - distance: colocalization distance [nm]
      - n_subcol: number of colocalizations (subcolumns)
      - n_set{0, 1, 2}_subcol: number of particles of a set in subcolumns
      - n_set{0, 1, 2}_total: total number of particles of a set  
      - n_col: number of columns (formed by subcolumns)
      - area, volume: total area [nm^2], volume [pixel]  of the voi where 
      particles are projected
      - area_col: area of the voi (as above) occupied by columns
      - n_subcol_random_all: number of subcolumns by random simulations
      where particles of set0 is kept fixed and particles of sets 1 and 2 
      are randomly distributed
      - n_subcol_random_alt_all: number of subcolumns by random simulations
      where particles of sets 1 and 2 are kept fixed and particles of set0 
      are randomly distributed
      - n_set{0, 1, 2}_subcol_random (only individual tomo table)
      - n_teth_cent, n_sv: should not be used
      
    Additional columns generated during preprocessing:
      - n_subcol_random_{mean, std}: mean, std of n_subcol_random_all
      - n_subcol_random_alt_{mean, std}: mean, std of n_subcol_random_alt_all
      - n_subcol_random_combined_{mean, std}: : mean, std for 
      n_subcol_random_all and n_subcol_random_alt_all taken together
      - p_subcol_normal: p-value for random simulations (fraction of 
      n_subcol_random_all smaller than n_subcol)
      - p_subcol_other: p-value for random_alt simulations (fraction of 
      n_subcol_random_alt_all smaller than n_subcol)
      - p_subcol_combined: p-value for random and random_alt simulations 
      taken together
      - density (only combined tomos):

    Columns generated by combine_permuted():
      - p_subcol_{normal, other, combined}_3: p-values for normal, other
      and combined cases like above, but here colocalizations obtained 
      from the same particle sets but in all available permuted order
      are used
    """

    def __init__(
            self, dir_=None, pick=None, dir_prefix='tables_', save_formats=None,
            join_suffix='data', individual_suffix='data_syn',
            group_suffix='data_group', mode=None):
        """
        Sets parameters.

        Arguments:
          - dir_: tables directory
          - pick: name of this colocalization project (used if dir_ is None)
          - dir_prefix: tables directory prefix
          - join_suffix: suffix given to colocalization data tables that 
          contain data for all tomos together
          - individual_suffix: suffix given to colocalization data tables that 
          contain data for individual tomos
          - mode: defines how to form paths to the raw colocalization 
          data and defines how to extract tomo id, use 'munc13_lite' for
          colocalizations calculated by ColocLite and see extract_data() doc
          for colocalizations generated by pyseg 
          - save_formats: (list) formats in which tables are saved 
          (see pyto.io.PandasIO)
        """

        # suffices given to colocalization data tables that contain data for
        # all tomos together and for individual tomos
        self.join_suffix = join_suffix
        self.individual_suffix = individual_suffix
        self.group_suffix = group_suffix

        # function used to determine p-values
        self.p_func = np.greater

        # file organization mode
        self.mode = mode

        # table save formats
        self.save_formats = save_formats
        
        # resolve tables directory, if possible
        if dir_ is None:
            if pick is not None:
                self.tables_dir = dir_prefix + pick
        else:
            self.tables_dir = dir_

    def copy_setup(self):
        """
        Returns an instance of this class that has the same attributes
        as this instance, except that it does not have any data attribute
        nor self._names.
        """
        
        new = self.__class__()
        new.join_suffix = self.join_suffix
        new.individual_suffix = self.individual_suffix
        new.group_suffix = self.group_suffix
        new.p_func = self.p_func
        new.mode = self.mode
        try:
            new.tables_dir = self.tables_dir
        except AttributeError: pass
            
        return new
            
    #
    # Class methods that run pyseg colocalization, read and preprocess
    # raw colocalization data (pyseg and ColocLite) are in super classes
    #

    ###########################################################
    #
    # Basic data manipulation
    #

    def get_data(self, name):
        """
        Returns joined tomos and individual tomos data tables for the given 
        colocalization name (arg name).

        Argument:
          - name: a single colocalization name (like 'setX_setY'), or a list of 
          particles sets (like ['setX', 'setY']

        Returns: 
          (data_table_all_tomos together, data_table_individual_tomos)
        """

        if isinstance(name, (list, tuple)):
            name_join, name_individual = self.make_table_names(layers=name)
        else:
            name_join = self.get_join_name(name)
            name_individual = self.get_individual_name(name)
            
        join_data = getattr(self, name_join)
        indiv_data = getattr(self, name_individual)

        return join_data, indiv_data

    def get_group_data(self, name, group_suffix=None):
        """Returns group data.
        """
        group_name = self.get_group_name(name=name, group_suffix=group_suffix)
        res = getattr(self, group_name)
        return res
    
    def get_join_name(self, name):
        """
        Returns name of the attribute holding data for all tomos together
        """
        return name + '_' + self.join_suffix
    
    def get_individual_name(self, name):
        """
        Returns name of the attribute holding data for individual tomos
        """
        return name + '_' + self.individual_suffix

    def get_group_name(self, name, group_suffix=None):
        """
        Returns name of the attribute holding data foor tomo groups
        """
        if group_suffix is None:
            try:
                group_suffix = self.group_suffix
            except AttributeError:
                group_suffix = None
            if group_suffix is None:
                raise ValueError(
                    "Arg group_suffix or self.group_suffix has to be "
                    + "specified")      
        return name + '_' + group_suffix

    def make_table_names(self, layers):
        """
        Makes standard table names from layer (particle set) names.

        Specifically, the name of the table where the data of all tomograms 
        is combined is:

          layers[0]_layers[1]..._join_suffix

        and the name of the table containing the data of all individual 
        tomograms is:

          layers[0]_layers[1]..._individual_suffix

        Arguments:
          - layers: (list) layer (particle set) names

        Needs:
          - self.join_suffix: suffix for tables containing combined data
          - self.individual_suffix: suffix for individual tomograms table

        Returns [combined_tomo_data_table, individual_tomo_data_table]
        """
        common = ''
        for lay in layers:
            common += lay + '_'
        result = (common + self.join_suffix, common + self.individual_suffix)
        return result

    def add_data(self, name, data, data_syn):
        """
        Adds specified data 
        """
        join_name = self.get_join_name(name=name)
        individ_name = self.get_individual_name(name=name)
        setattr(self, join_name, data)
        setattr(self, individ_name, data_syn)
        try:
            self._names.append(name)
        except (NameError, AttributeError):
            self._names = [name]
            
        
    ###########################################################
    #
    # Methods needed to read and preprocess data
    #

    def add_random_stats(self, names=None, n_particles=False):
        """
        Calculates basic statistics (mean and std) for random simulations
        and adds it to the data tables, for multiple 3-colocalizations
        at all distances.

        Requires that data tables are already present as attributes of this 
        instance.

        Note: The functionality this method provides is included 
        extract_data(), but there are two important differences. First, this 
        method takes multiple colocalization names as arguments, while 
        extract_data() works only on one colocalization. Second, this
        method requires data tables, while extract_data() works on raw data.

        Argument:
          - names: list of colocalization names
          - n_particles: flag indicating wheteher stats for number of 
          particles in subcolumns is calculated

        Random data columns:
          - n_subcol_random_all
          - n_subcol_random_alt_all
          - particles in subcolumns for all three layers separately (if
          n_particles is True)

        The data is added to both individual synapses and synapses 
        taken together data tables.
        """

         # get names
        if names is None: names = self._names
        
        # loop over all colocalizations
        for nam in names:

            # get data and initialize for this colocalization
            data_name = self.get_join_name(nam)
            data_syn_name = self.get_individual_name(nam)
            data = getattr(self, data_name)
            data_syn = getattr(self, data_syn_name)

            # n subcolumn
            data, data_syn = col_func.get_random_stats(
                data=data, data_syn=data_syn, column='n_subcol_random_all', 
                out_column='n_subcol_random', combine=False)
            data, data_syn = col_func.get_random_stats(
                data=data, data_syn=data_syn, column='n_subcol_random_alt_all', 
                out_column='n_subcol_random_alt', combine=False)
            data, data_syn = col_func.get_random_stats(
                data=data, data_syn=data_syn, 
                column=['n_subcol_random_all','n_subcol_random_alt_all'], 
                out_column='n_subcol_random_combined', combine=True)

            # particles in subcolumn
            if n_particles:
                pset_names = col_func.get_layers(name=nam)
                for layer_name in set(pset_names):
                    random_col_name = 'n_' + layer_name + '_subcol_random'
                    data, data_syn = col_func.get_random_stats(
                        data=data, data_syn=data_syn, column=random_col_name, 
                        out_column=random_col_name, combine=False)

            # save data
            setattr(self, data_name, data)
            setattr(self, data_syn_name, data_syn)


    
    ####################################################
    #
    # Colocalization analysis (data processing) of preprocessed
    # coloclization data
    #

    def full_split_by_groups(
            self, id_group, group_label, id_label,
            distance=None, p_values=True, random_stats=False):
        """Splits all colocalization data in tomo groups.

        Extracts and returns data for all colocalizations of this 
        instance (both the individual and all tomos tables) separated in the
        specified groups of tomograms. These groups are specified by another 
        table (args id_group, group_name group_label and id_label). The 
        returned data contains only the specified distances (arg distance). 

        This method extends (and is based on) select_by_group(). There are 
        two differences:
          - This method extract data for all colocalizations
          present in this instance
          - This method extracts colocalization data for all group names
          existing in id_labels table and returns separate objects for
          each group.

        For all other details please see select_by_group() and select() 
        methods.

        Arguments:
          - id_group: (DataFrame) table that contains group names and tomo ids
          - group_label: name of the column of id_group table that contains
          group names
          - id_label: name of the column of id_group table that contains
          tomo ids
          - distance: list of distances in nm for which the data 
          is calculated, if None (default) all distances are used
          - p_values: flag indication if p_values should be calculated
          - random_stats: flag indicating whether basic stats are calculated
          for random simulations

        Returns dictionary where keys are group names and values are 
        corresponding instances of this class. If a group of the id_group
        table does not exist in this instance, the value of the returned
        dictionary for that group is set to None.
        """

        # loop over groups
        result = {}
        g_names = id_group[group_label].unique()
        for g_nam in g_names:

            # make object for this group and set data
            curr_obj = self.copy_setup()
            found = False
            for name in self._names:
                data, data_sep = self.select_by_group(
                    name=name, id_group=id_group, group_name=g_nam,
                    group_label=group_label, id_label=id_label,
                    distance=distance,
                    p_values=p_values, random_stats=random_stats)
                curr_obj.add_data(name=name, data=data, data_syn=data_sep)
                if (data is not None) and (data_sep is not None):
                    found = True
            if not found:
                curr_obj = None
            result[g_nam] = curr_obj

        return result
        
    def split_by_groups(
            self, name, id_group, group_label, id_label,
            distance=None, p_values=True, random_stats=False):
        """Splits data of one colocalization according to tomo groups.

        This method extracts and returns data for the specified 
        colocalization (arg name) of this instance for each tomo group. 
        That is the data of individual tomos belonging to a group are joined
        together. Both the individual and all tomos tables are extracted 
        and returned.

        Tomo groups are defined by different values present in the specified 
        column (arg group_label) of the specified table (arg id_group). Each 
        tomo group may contain one or more tomos.

        Tomo ids are read from another column (arg id_label) of the same table 
        (arg id_group).

        The returned data is restricted to the specified distances.

        This method extends (and is based on) select_by_group() except that
        it extracts colocalization data for all group names
        existing in id_labels table and returns separate objects for
        each group.

        For all other details please see select_by_group() and select() 
        methods.

        Arguments:
          - name: name of the colocalization
          - id_group: (DataFrame) table that contains group names and tomo ids
          - group_label: name of the column of id_group table that contains
          group names
          - id_label: name of the column of id_group table that contains
          tomo ids
          - distance: list of distances in nm for which the data 
          is calculated, if None (default) all distances are used
          - p_values: flag indication if p_values should be calculated
          - random_stats: flag indicating whether basic stats are calculated
          for random simulations

        Returns (all_tomos_dict, individual_tomos_dict), where the two 
        returned dictionaries have group names as keys and the all tomos
         and individual tomos tables, respectively, as values.
        """

        # loop over groups, select tomos
        data_dict = {}
        data_sep_dict = {}
        g_names = id_group[group_label].unique()
        for g_nam in g_names:
            data, data_sep = self.select_by_group(
                name=name, id_group=id_group, group_name=g_nam,
                group_label=group_label, id_label=id_label, distance=distance,
                p_values=p_values, random_stats=random_stats)
            data_dict[g_nam] = data
            data_sep_dict[g_nam] = data_sep

        return data_dict, data_sep_dict
            
    def select_by_group(
            self, name, id_group, group_name, group_label, id_label,
            distance=None, p_values=True, random_stats=False):
        """Extracts data of one colocalization for a group of tomos.

        Extracts and returns data for one colocalization (both 
        the individual and all tomos tables) that contain only the data 
        for tomos specified by another table (args id_group, group_name
        group_label and id_label) and distances (arg distance). 

        The colocalization data is obtained from the individual 
        tomos table of the specified colocalization (arg name).

        This method is the same as (and is based on) select(), except
        that tomo selection is done based on another table (arg id_group)
        that contains group names and tomo ids. In short, tomo ids are 
        found that belong to the specified group (arg group_name).

        More precisely, all rows of the (arg) id_group table that have 
        (arg) group_name value in column (arg) group_label are selected,
        and the tomo ids given in column (arg) id_label of the selected 
        rows are extracted. Method select is then called using the extracted
        tomo ids.
        
        If none of the specified ids exist, (None, None) is returned.

        Arguments:
          - name: name of the colocalization
          - id_group: (DataFrame) table that contains group names and tomo ids
          - group_name: group name
          - group_label: name of the column of id_group table that contains
          group names
          - id_label: name of the column of id_group table that contains
          tomo ids
          - distance: list of distances in nm for which the data 
          is calculated, if None (default) all distances are used
          - p_values: flag indication if p_values should be calculated
          - random_stats: flag indicating whether basic stats are calculated
          for random simulations

        Returns: all_tomos_table, individual_tomos_table
        """

        # get tomo ids for the specified group 
        if isinstance(id_group, pd.DataFrame):
            ids = id_group[id_group[group_label]==group_name][id_label]
            ids = ids.unique()
        else:
            raise ValueError("Argumnent id_group has to be pandas.DataFrame") 

        # select data 
        data, data_sep = self.select(
            name=name, ids=ids, distance=distance, p_values=p_values,
            random_stats=random_stats)
            
        return data, data_sep
       
    def select(
            self, name, ids=None, distance=None, p_values=True,
            random_stats=False):
        """Extracts data of one colocalization for a group of tomos.

        Extracts and returns data for one colocalization (both 
        the individual and all tomos tables) that contain only the data 
        for tomos specified by tomo ids (arg ids) and distances (arg 
        distance). 

        The colocalization data is obtained from the individual 
        tomos table of the specified colocalization (arg name).

        This instance has to contain both individual and joined data 
        tables corresponding to arg name. These are returned by 
        self.get_data(name). 

        The extracted individual tomos table is generated by simply
        selecting the rows of the original individual tomos table.

        Values of  extracted all tomos table are calculated from the 
        extracted individual tomos table. Some columns (like number of 
        subcolumns or number of particles in subcolumns) are obtained
        by simple summation. Values of other columns (like p-values and
        simulation means and stds) are recalculated from the extracted 
        individual tomos table.

        The individual tomos table (pandas.DataFrame specified by arg name) 
        has to be indexed by (tomo) ids or have a column named 'id' that 
        contains ids.

        If none of the specified ids exist, (None, None) is returned.

        Arguments:
          - name: name of the colocalization
          - ids: list of tomo ids
          - distance: list of distances in nm for which the data 
          is calculated, if None (default) all distances are used
          - p_values: flag indication if p_values should be calculated
          - random_stats: flag indicating whether basic stats are calculated
          for random simulations

        Returns: all_tomos_table, individual_tomos_table
        """

        # get individual tomos data table
        data_orig, data_orig_sep = self.get_data(name)

        # select specified rows
        data_sep = col_func.select_rows(
            data=data_orig_sep, ids=ids, distance=distance)
        if data_sep.shape[0] == 0:
            return None, None

        # get distances
        if distance is None:
            distance = data_sep['distance'].unique()
            distance.sort()

        # combine data from selected tomos
        if (self.mode is not None) and (self.mode == 'munc13_lite'):
            add_columns, array_columns = col_func.get_aggregate_columns(
                columns=data_sep.columns)
        else:
            _, _, add_columns, array_columns = col_func.set_read_parameters(
                name=name, distances=distance, in_path='_foo', mode=self.mode,
                n_sim=None)
        data_tog = col_func.aggregate(
            data=data_sep, distance=distance, add_columns=add_columns,
            array_columns=array_columns, p_values=p_values,
            p_func=self.p_func, random_stats=random_stats)

        # add columns of original that are not in data_tog, try to
        # preserve order
        orig_only = [
            col for col in data_orig.columns if col not in data_tog.columns]
        orig_only_pos = [data_orig.columns.get_loc(col) for col in orig_only]
        new_cols = data_tog.columns.to_list()
        data = pd.merge(
            left=data_orig[['distance']+orig_only], right=data_tog,
            validate='one_to_one')
        for pos, col in zip(orig_only_pos, orig_only):
            new_cols.insert(pos, col)
        data = data[new_cols]
        
        # return       
        return data, data_sep
                

    ####################################################
    #
    # Colocalization analysis (data processing) of preprocessed
    # coloclization data
    #

    def calculate_ring_density(
            self, names, layer_ind, distances, 
            normalize=False, normalize_index=0):
        """Calculates circular ring density of one colocalized particle set. 

        The calculation is done for multiple colocalization cases.

        Calculates number of particles in a ring divided by ring
        surface area for each ring of one or more colocalizations 
        (arg names). The rings are formed by circles
        with radii obtained from consecutive distances (arg distances).

        For example, if distances = [5, 10, 15], the density is calculated 
        for the circle of radius 5 nm, and rings 5-10 nm and 10-15 nm.

        Note that the number of particles divided by ring surface area
        has dimensions of but it is not really area concentration
        because the number is divided by surface area of one ring and not
        by the total number of rings. 

        Colocalization data tables (pandas.DataFrame) have to be 
        defined before in the module (object) specified by arg module.
        The corresponding variable names have to be of the form 
        colocalization name + '_data'.

        Arg layer_index defines the particle set (layer) for which ring 
        surface density is calculated. For example, if name is 
        pre_tether_post, layer_index 1 selects tether particles and 
        2 post particles.

        Arguments:
          - names: list of colocalization names
          - layer_index: position within a colocalization name the defines 
          the particle set whose ring surface density is calculated 
          - distances: list of distances
          - module: module where colocalization data tables are defined
          - normalize: if True, the density values are normalized to the
          surface density value of the specified ring (by arg normalize_index)
          - normalize_index: index of the ring that is used for normalization

        Returns: (pd.DataFrame) Table containing the calculated ring surface
        densities where columns are colocalizations (arg names) and rows
        are rings.
        """

        # set module
        #if module is None:
        #    module = sys.modules[__name__]

        # initialize resulting table
        distances_0 = [0] + distances
        ring_labels = [
            '{}-{}'.format(distances_0[i], distances_0[i+1]) 
            for i in range(len(distances))]    
        result = pd.DataFrame({'ring' : ring_labels})
        #result = result.set_index('ring')  bad graph points ordering

        # loop over all colocalizations
        for nam in names:

            # get data and initialize for this colocalization
            try:
                data = self.get_data(nam)[0]
            except AttributeError:
                continue
            column_name = (
                'n_' + col_func.get_layers(nam)[layer_ind] + '_subcol')
            n_part = data.loc[:, column_name]
            n_part_0 = np.insert(np.array(n_part), 0, 0)
            density = np.zeros(len(distances)) - 1

            # caluclate ring density
            for index in range(len(distances)):
                n_diff = n_part_0[index+1] - n_part_0[index]
                surface = (
                    np.pi * (distances_0[index+1]**2 - distances_0[index]**2))
                density[index] = n_diff / surface
            if normalize:
                density = density / density[normalize_index]

            # put this ring density as one column
            result[nam] = density

        return result

    def plot_ring_density(
            self, names, distances, density=None, layer_ind=None, 
            normalize=False, normalize_index=0,
            ax=None, labels=None, bare=False):
        """
        Plots number of particles in a ring divided by ring
        surface area for each ring of one or more colocalizations 
        (arg names). The rings are formed by circles
        with radii obtained from consecutive distances (arg distances).
        The ring surface densities can be calculated or specified by 
        arg density.

        For example, if distances = [5, 10, 15], the density is calculated 
        for the circle of radius 5 nm, and rings 5-10 nm and 10-15 nm.

        Note that the number of particles divided by ring surface area
        has dimensions of but it is not really area concentration
        because the number is divided by surface area of one ring and not
        by the total number of rings. 

        Arg layer_index defines the particle set whose ring surface density 
        is calculated. For example, if name is pre_tether_post, layer_index
        1 selects tether particles and 2 post particles.

        Colocalization data tables (pandas.DataFrame) have to be 
        defined before in the module (object) specified by arg module.
        The corresponding variable names have to be of the form 
        colocalization name + '_data'.

        If an existing graph is specified by arg ax, the densities are
        plotted on that graph. Otherwise a new graph is made.
        If arg bare is False, graph legend and axis label are added to 
        the graph. 

        Consequently, ax=None and bare=True are used to plot a single 
        graph. For plotting densities of multiple colocalizations
        (that cannot be specified by arg names) the first plot
        should have ax=None and the other the value returned by the
        prvious call, while bare=False should be set only for one call.

        Arguments:
          - names: list of colocalization names
          - layer_index: position within a colocalization name the defines 
          the particle set whose ring surface density is calculated 
          - distances: list of distances
          - density: (pd.DataFrame) ring densities to be plotted, in the 
          same form as the calculated (returned) ring surface densities
          - module: module where colocalization data tables are defined
          - normalize: if True, the density values are normalized to the
          surface density value of the specified ring (by arg normalize_index)
          - normalize_index: index of the ring that is used for normalization
          - ax: an existing graph, or None for a new one
          - bare: Indicates whether graph legend and axis labels are added 
          - labels: (dict) graph labels for colocalizations where keys are 
          colocalization names and values the corresponding labels

        Returns: 
          - ax: this plot
          - table (pd.DataFrame): Table containing the calculated ring 
          surface densities where rows are colocalizations (arg names) 
          and columns are rings.
        """

        # set module
        #if module is None:
        #    module = sys.modules[__name__]

        # initialize 
        if ax is None:
            fig, ax = plt.subplots()
        distances_0 = [0] + distances

        # calculate ring density if not specified
        if density is None:
            density = self.calculate_ring_density(
                names=names, layer_ind=layer_ind, 
                distances=distances, normalize=normalize, 
                normalize_index=normalize_index)

        # plot data on graph
        for nam in names:

            x_ind = range(len(distances))
            if labels is not None:
                lab = labels[nam]
            else:
                lab = nam
            ax.plot(density.index, density[nam], 'x', linestyle='-', label=lab)

        if bare:
            return ax, density

        # add labels and related to the graph
        ax.legend(loc='best')
        ax.set_xlabel('Distance [nm]')
        if normalize:
            ax.set_ylabel('Normalized n particles per ring area')
        else:
            ax.set_ylabel('N particles per ring area [$1/nm^2$]')
        ax.set_xticks(x_ind)
        ax.set_xticklabels(
            ['{}-{}'.format(distances_0[i], distances_0[i+1]) 
             for i in range(len(distances))] )

        return ax, density

    def combine_23(self, name, distance):
        """
        Makes a table that contains colocalization data for one
        3-colocalization and the three corresponding 2-colocalizations, all
        for one distance.

        Colocalization data tables (pandas.DataFrame) have to be 
        defined before as attributes of this object. The corresponding 
        variable names have to be of the form colocalization name + '_data'.

        Arguments:
          - name: 3-colocalization name
          - distance: distance
          - module: module where colocalization data tables are defined

        Returns: (pd.DataFrame) Table where rows correspond to the four 
        colocalizations and the columns are the union of columns of the
        colocalizations.

        """
        pset_names = col_func.get_layers(name=name)
        data_3 = self.get_data(name)[0]
        data_3['name'] = name

        name_01 = pset_names[0] + '_' + pset_names[1]
        data_01 = self.get_data(name_01)[0]
        data_01['name'] = name_01

        name_02 = pset_names[0] + '_' + pset_names[2] 
        data_02 = self.get_data(name_02)[0]
        data_02['name'] = name_02

        name_12 = pset_names[1] + '_' + pset_names[2] 
        data_12 = self.get_data(name_12)[0]
        data_12['name'] = name_12

        data = pd.concat(
            [data_3, data_01, data_02, data_12], ignore_index=True, sort=False)
        data = data[data['distance'] == distance].copy()
        data = data.set_index('name')

        return data

    def calculate_ring_density_23(
            self, name, distances, normalize, normalize_index=0, 
            subtract=False):
        """Calculates circular ring density of one complete 3-colocalization.

        The calculation is done for a single 3-colocalization case and
        all associated 2 colocalizations. Specifically:
          - particles of layer 1 for the 3-colocalization
          - particles of layer 2 for the 3-colocalization
          - particles of layer 1 for 2-colocalization between layers 0 and 1
        that are not in the 3-colocalization
          - particles of layer 2 for 2-colocalization between layers 0 and 2
        that are not in the 3-colocalization
          - particles of layer 2 for  2-colocalization between layers 1 and 2
        Layers in a 3-colocalization are labeled from 0 to 2.

        If arg subtract is True, 3-coloc ring densities are subtracted from 
        the corresponding 2-coloc. In this way, the resulting 2-coloc
        ring densities are exclusive, that is they show 2-colocalizations
        that are not present in 3-colocalizations.

        For the calculation of one circular ring density see docs for
        calculate_ring_density().

        Returns:
           - density: Table containing the calculated densities
         """

        # particle set and 2-colocalization names
        psets = col_func.get_layers(name)
        names_2c = [
            psets[0] + '_' + psets[1], psets[0] + '_' + psets[2],
            psets[1] + '_' + psets[2]]

        # calculate densities for 3-colocs
        density_1 = self.calculate_ring_density(
            names=[name], layer_ind=1, distances=distances, 
            normalize=normalize, normalize_index=normalize_index)
        density_2 = self.calculate_ring_density(
            names=[name], layer_ind=2, distances=distances, 
            normalize=normalize, normalize_index=normalize_index)

        # calculate subtracted densities for 2-colocs
        density_2c = self.calculate_ring_density(
            names=names_2c, layer_ind=1,
            distances=distances, normalize=normalize, 
            normalize_index=normalize_index)

        # subtract 2-coloc from 3-coloc densities if needed 
        if subtract:
            density_2c[names_2c[0]] = density_2c[names_2c[0]] - density_1[name]
            density_2c[names_2c[1]] = density_2c[names_2c[1]] - density_2[name]
            #density_2c[names_2c[2]] = density_2c[names_2c[2]] - density_2[name]

        # rename 3-coloc columns
        density_1 = density_1.rename(columns={name : psets[1] + ': ' + name})
        density_2 = density_2.rename(columns={name : psets[2] + ': ' + name})

        # rename 2-coloc columns
        layer_indices = [1, 2, 2]
        labels_2c = dict(
            [(nam, '{}: {}'.format(psets[l_ind], nam)) 
                for nam, l_ind in zip(names_2c, layer_indices)])  
        density_2c = density_2c.rename(columns=labels_2c)

        # put all densities into a single table
        densities = pd.concat([density_1, density_2, density_2c], axis=1)
        densities = densities.iloc[:,[0,1,3,5,6,7]].copy()

        return densities

    def plot_ring_density_23(
            self, name, distances, normalize, normalize_index=0, 
            subtract=False):
        """
        Calculates and plots number of particles in a ring divided by ring
        surface area (ring density) for each ring of the specified 
        3-colocalization (arg name) and for the corresponding 2-coloclazations.

        Note that this ring density has dimensions of but it is not really 
        area concentration because the number is divided by surface area of 
        one ring and not by the total number of rings. 

        The ring density is calculated for the following:
          - particles of layer 1 for the 3-colocalization
          - particles of layer 2 for the 3-colocalization
          - particles of layer 1 for 2-colocalization between layers 0 and 1
        that are not in the 3-colocalization
          - particles of layer 2 for 2-colocalization between layers 0 and 2
        that are not in the 3-colocalization
          - particles of layer 2 for  2-colocalization between layers 1 and 2

        Layers in a 3-colocalization are labeled from 0 to 2.

        Returns:
          - ax: axis
          - density: Table containing the calculated densities
        """

        # calculate densities
        densities = self.calculate_ring_density_23(
            name=name, distances=distances, normalize=normalize, 
            normalize_index=normalize_index, subtract=subtract)

        columns = [col for col in densities.columns if col != 'ring']
        labels = dict([(col_nam, col_nam) for col_nam in columns])
        if subtract:
            labels[columns[2]] = columns[2] + ' - 3coloc' 
            labels[columns[3]] = columns[3] + ' - 3coloc' 

        ax, _ = self.plot_ring_density(
            names=columns, density=densities, distances=distances, 
            normalize=normalize, normalize_index=normalize_index,
            labels=labels)

        return ax, densities

    def calculate_fraction_particles_single(self, name, plot=False):
        """
        Calculates fraction of particles present in the specified subcolumn 
        (3-colocalization)
        """

        # set module
        #if module is None:
        #    thismodule = sys.modules[__name__]

        data = self.get_data(name)[0]
        data = data.copy()

        if plot:
            fig, ax = plt.subplots()

        pset_names = col_func.get_layers(name=name)
        for layer_name in set(pset_names):

            subcol_column = 'n_' + layer_name + '_subcol'
            total_column = 'n_' + layer_name + '_total'
            fract_column = 'n_' + layer_name + '_fraction'
            data[fract_column] = data[subcol_column] / data[total_column]

            if plot:
                ax.plot(
                    'distance', fract_column, 'x', data=data, linestyle='-', 
                    label=fract_column)

        if plot:
            return ax, data
        else:
            return data

    def plot_fraction_particles_single(self, name):
        """
        Plots fraction of particles present in specified subcolumn 
        (3-colocalization)
        """
        return self.calculate_fraction_particles_single(
            name=name, plot=True)

    def get_subcol_content(self, names, distance):
        """
        Calculates the number of tether, pre and post complexes contained 
        within specified subcolumns (3-colocalizations) per subcolumn.

        Arguments:
          - names: list of 3-colocalization names
          - distance: distance
          - module: module where the 3-colocalization data are defined  

        Returns: DataFrame containing the means
        """

        result = None
        for nam in names:

            # get column names
            pre_name, tether_name, post_name = col_func.get_layers(nam)
            subcol_name = 'n_subcol'
            tether_sc_name = 'n_' + tether_name + '_subcol'
            pre_sc_name = 'n_' + pre_name + '_subcol' 
            post_sc_name = 'n_' + post_name + '_subcol'

            # get data
            data = self.get_data(nam)[0].copy()
            data = data[data.distance==distance]

            # calculate
            row = pd.DataFrame(
                np.array(
                    data[[pre_sc_name, tether_sc_name, post_sc_name]] 
                    / data[subcol_name].squeeze()),
                columns=['pre_per_sc', 'tether_per_sc', 'post_per_sc'],
                index=[nam])

            # add to resulting table
            try:
                #result = result.append(row, sort=False)
                result = pd.concat([result, row], ignore_index=True)
            except (NameError, AttributeError):
                result = row

        return result

    def get_reaction_states(self, name, distances, subtract=True):
        """
        """

        # get names
        names_all = col_func.get_layers(name=name)
        data_3_sc_cols = (
            ['n_subcol'] + ['n_' + name_x + '_subcol' for name_x in names_all])
        name_01 = names_all[0] + '_' + names_all[1]
        data_01_sc_cols = (
            ['n_subcol'] + 
            ['n_' + name_x + '_subcol' for name_x in names_all[:2]])
        name_02 = names_all[0] + '_' + names_all[2]
        data_02_sc_cols = (
            ['n_subcol'] + ['n_' + name_x + '_subcol' 
                            for name_x in [names_all[0], names_all[2]]])

        data_3 = self.get_reaction_states_single(
            name=name, distances=distances)
        data_01 = self.get_reaction_states_single(
            name=name_01, distances=distances)
        data_02 = self.get_reaction_states_single(
            name=name_02, distances=distances)

        if subtract:
            d_01 = data_01.loc[data_01.index, data_01_sc_cols]
            d_3 = data_3.loc[data_3.index, data_01_sc_cols]
            data_01.loc[data_01.index, data_01_sc_cols] = d_01 - d_3

            d_02 = data_02.loc[data_02.index, data_02_sc_cols]
            d_3 = data_3.loc[data_3.index, data_02_sc_cols]
            data_02.loc[data_02.index, data_02_sc_cols] = d_02 - d_3

        data =  pd.concat(
            [data_3, data_01, data_02], ignore_index=True, sort=False)
        return data

    def get_reaction_states_single(self, name, distances):
        """
        """

        names_all = col_func.get_layers(name=name)

        data_3 = self.get_data(name)[0]
        data_3_sc_cols = (
            ['n_subcol'] + ['n_' + name_x + '_subcol' for name_x in names_all])
        data_3_total_cols = ['n_' + name_x + '_total' for name_x in names_all]
        data_3_cols = data_3_sc_cols + data_3_total_cols

        dists = [0] + distances
        for dist_ind in range(1, len(dists)):

            # copy values for subcolumns
            data_loc = data_3[
                data_3['distance'] == dists[dist_ind]][data_3_cols].copy()
            data_loc['name'] = name
            data_loc['ring'] = '{}-{}'.format(
                dists[dist_ind-1], dists[dist_ind]) 
            data_loc = data_loc[['name', 'ring'] + data_3_cols]
            data_previous_tmp = data_loc.copy()

            # calculate values for rings
            try:
                this = data_loc.loc[data_loc.index[0], data_3_sc_cols]
                prev = data_previous.loc[data_previous.index[0], data_3_sc_cols]
                data_loc.loc[data_loc.index[0], data_3_sc_cols] = this - prev
            except (NameError, AttributeError):
                pass
            data_previous = data_previous_tmp

            # append to data table
            try:
                #data = data.append(data_loc, ignore_index=True)
                data = pd.concat([data, data_loc], ignore_index=True)
            except (NameError, AttributeError):
                data = data_loc.copy()

        return data

    def plot_ring_numbers(
            self, name, distances, layer_index, ring_normalize=False, 
        labels=None, ax=None):
        """
        """

        # get rings data
        rings = self.get_reaction_states(name=name, distances=distances)
        layer_names = col_func.get_layers(name)

        # calculate ring area
        circle_area = np.pi * np.array(distances) ** 2
        inner_circle_area = np.hstack([[0], circle_area[:-1]])
        ring_area = circle_area - inner_circle_area

        if ax is None:
            fig, ax = plt.subplots()

        # 3-colocalization
        col_name = name
        column = 'n_{}_subcol'.format(layer_names[layer_index])
        if labels is None:
            label = '{} 3-col'.format(layer_names[layer_index])
        else:
            label = labels.pop(0)
        rings_one = rings[rings['name']==col_name].copy().reset_index()
        y_data = rings_one[column]
        if ring_normalize:
            y_data = y_data / ring_area
        ax.plot(rings_one.index, y_data, 'x', linestyle='-', label=label)

        # 2-colocalization
        col_name = '{}_{}'.format(layer_names[0], layer_names[layer_index])
        column = 'n_{}_subcol'.format(layer_names[layer_index])
        if labels is None:
            label = '{} 2-col'.format(layer_names[layer_index])
        else:
            label = labels.pop(0)
        rings_one = rings[rings['name']==col_name].copy().reset_index()
        y_data = rings_one[column]
        if ring_normalize:
            y_data = y_data / ring_area
        ax.plot(rings_one.index, y_data, 'x', linestyle='-', label=label)

        # finish plot
        if ring_normalize:
            ax.set_ylabel('Ring normalized N particles [$1/nm^2$]')
        else:
            ax.set_ylabel('Number of particles')
        ax.set_xticks(list(range(rings_one.shape[0])))
        ax.set_xticklabels(rings_one['ring'])
        ax.set_xlabel('Distance [nm]')
        ax.legend(loc='best');

        return ax, rings

    def make_2coloc_significance_table(
            self, names_1, names_2, names_1_col, target_col='distance', 
            target_values=None,  signif_col='p_subcol_combined', 
            signif_value=0.95):
        """
        Considers all 2-colocalization that can be formed between particle 
        sets specified by args names_1 and names_2. For each of them, 
        finds all values of the target column (specified 
        by arg target_col), for which the significance column (specified by 
        arg signif_col) has value greater than arg signif_value. 

        If arg target_values is not None, only the colocalizations (table
        rows) that have one of the specified target_values in target_col 
        column are considered.

        Common usage: To return table of distances for which number of 
        2-colocalizations is significant to >95% agains the combined 
        random model:

        self.make_2coloc_significance_table(
            namea_1=pre_names, name_2=tether_names+post_names,
            names_1_col='pre', target_col='distance', target_values=None, 
            signif_col='p_subcol_combined', signif_value=0.95)

        Arguments:
          - names_1, names_2: (list) particles sets of layers 1 and 2
          - names_1_col: name of the first (index) column of the table
          - target_col: name of the target column
          - target_values: list or array of target values, or None for 
          all values
          - signif_col: name of the significance column
          - signif_value: threshold significance value 

        Returns: (pandas.DataFrame) table containing selected target values
        """

        # initialize
        res_table = pd.DataFrame()

        # loop over columns defined by names_2
        for nam_2 in names_2:

            # make all values for this column 
            col_data = []
            for nam_1 in names_1:
                #name = nam_1 + '_' + nam_2 + '_data'
                name = self.get_join_name(f'{nam_1}_{nam_2}')
                col_data.append(self.find_significant(
                    name=name, target_col=target_col, 
                    target_values=target_values, signif_col=signif_col, 
                    signif_value=signif_value))

            # add this column to the resulting table
            col_table = pd.DataFrame(
                {names_1_col : names_1, nam_2 : col_data})
            col_table = col_table.set_index(names_1_col)
            res_table = pd.concat([res_table, col_table], axis=1)

        return res_table

    def make_3coloc_significance_table(
            self, names_1, names_2, names_3, names_1_col, 
            target_col='distance', 
            target_values=None,  signif_col='p_subcol_combined', 
            signif_value=0.95):
        """
        Considers all 3-colocalization that can be formed between particle 
        sets specified by args names_1, names_2 and names_3. For each of them, 
        finds all values of the target column (specified 
        by arg target_col), for which the significance column (specified by 
        arg signif_col) has value greater than arg signif_value. 

        If arg target_values is not None, only the colocalizations (table
        rows) that have one of the specified target_values in target_col 
        column are considered.

        Common usage: To return table of distances for which number of 
        3-colocalizations is significant to >95% agains the combined 
        random model:

        self.make_3coloc_significance_table(
            namea_1=pre_names, name_2=tether_names, names_3=post_names,
            names_1_col='pre', target_col='distance', target_values=None, 
            signif_col='p_subcol_combined', signif_value=0.95)

        Arguments:
          - names_1, names_2, names_3: (list) particles sets of layers 
          1, 2 and 3
          - names_1_col: name of the first (index) column of the table
          - target_col: name of the target column
          - target_values: list or array of target values, or None for 
          all values
          - signif_col: name of the significance column
          - signif_value: threshold significance value 

        Returns: (pandas.DataFrame) table containing selected target values
        """

        # initialize
        res_table = pd.DataFrame()

        # loop over columns defined by names_2
        for nam_2, nam_3 in itertools.product(names_2, names_3):

            # make all values for this column 
            col_data = []
            for nam_1 in names_1:
                #name = nam_1 + '_' + nam_2 + '_' + nam_3 + '_data'
                name = self.get_join_name(f'{nam_1}_{nam_2}_{nam_3}')
                col_data.append(self.find_significant(
                    name=name, target_col=target_col, 
                    target_values=target_values, signif_col=signif_col, 
                    signif_value=signif_value))

            # add this column to the resulting table
            col_table = pd.DataFrame(
                {names_1_col : names_1, nam_2+'_'+nam_3 : col_data})
            col_table = col_table.set_index(names_1_col)
            res_table = pd.concat([res_table, col_table], axis=1)

        return res_table

    def find_significant(
            self, name, target_col='distance', target_values=None, 
            signif_col='p_subcol_combined', signif_value=0.95):
        """
        Given a colocalization table specified by arg name, finds and 
        returns all values of the target column (specified by arg target_col),
        for which the significance column (specified by arg signif_col) 
        has value greater than arg signif_value. 

        If arg target_values is not None, only the colocalizations (table
        rows) that have one of the specified target_values in target_col 
        column are considered.

        Common usage: To return distances for which number of colocalizations
        is significant to >95% agains the combiner random model:

        self.find_significant(
            name=coloc_name, target_col='distance', target_values=None, 
            signif_col='p_subcol_combined', signif_value=0.95)

        Arguments:
          - name: colocalization name
          - target_col: name of the target column
          - target_values: list or array of target values, or None for 
          all values
          - signif_col: name of the significance column
          - signif_value: threshold significance value 

        Returns: ndarray of selected target values
        """

        # find all distances where significant
        data = getattr(self, name)
        sig_data = data[data[signif_col] > signif_value]
        result = np.array(sig_data[target_col])

        # restrict to specified distances
        if target_values is not None:
            result = np.array(
                [one_res for one_res in result if res_dist in target_values])

        return result

    def make_3coloc_table(
            self, names_1, names_2, names_3, select_value,
            select_col='distance', target_col='n_subcol',
            fraction=False, totals=False):
        """
        Considers all 3-colocalization that can be formed between particle 
        sets specified by args names_1, names_2 and names_3. Makes a table 
        that contains one value from each of the 3-colocalization.

        The selected value is found at the intersection of the column 
        specified by arg target_col and the row specified by args 
        select_col and select_value.

        Note: The select_value is expected to be unique in the select_col.

        Note: In the current implementation, arg names_2 should contain 
        only one element. 

        Arguments:
          - names_1, names_2, names_3: (list) particles sets of layers 
          1, 2 and 3
          - target_col: name of the target column
          - select_column: name of the column used to select one row
          - select_value: value in the select_column that selects one row
          - target_values: list or array of target values, or None for 
          - fraction: if True, the selected values are divided by the
          total number of the corresponding particles
          - totals: if True, totals (marginal values) are added for 
          all rows and columns

        Returns (pandas.DataFrame) table containing the selected values
        """

        # loop over columns
        table = pd.DataFrame()
        for post in names_3:

            # loop over rows
            column_values = []
            for pre in names_1:

                # get value for the selected row of this column
                name = pre + '_' + names_2[0] + '_' + post
                name = self.get_join_name(name)
                coloc_local = getattr(self, name)
                row = (coloc_local[select_col] == select_value)
                value = np.array(coloc_local.loc[row, target_col])[0]

                # normalize to total number, if needed
                if fraction:
                    pre_total_column = 'n_' + pre + '_total'
                    post_total_column = 'n_' + post + '_total'
                    pre_total = np.array(
                        coloc_local.loc[row, pre_total_column])[0]
                    post_total = np.array(
                        coloc_local.loc[row, post_total_column])[0]
                    value = value / float(pre_total * post_total)
                column_values.append(value)

            # add this column to resulting table
            column_tab = pd.DataFrame({'pre' : names_1, post : column_values})
            column_tab = column_tab.set_index('pre')
            table = pd.concat([table, column_tab], axis=1)

        # add totals if needed
        if totals:
            table.loc[:, 'total'] = table.sum(axis=1)
            table.loc['total', :] = table.sum(axis=0)

        return table

    def combine_permuted(
            self, names, p_columns=['p_subcol_normal', 'p_subcol_other'],
            out_column_suffix='_3', check_column='distance', 
            update=True, ignore=True, verbose=True):
        """
        Combines averages of p_values from all of the collocalization data 
        tables specifed by arg name and from other existing tables 
        obtained by permuting the order of particle sets comprising 
        specified table names.

        Based on combine_permuted_one().

        For each table (element of arg names), all different combinations 
        of the particle sets contained in the name of this table.

        Implemented for combined tomos tables only.

        Implemented for 3-colocalizations only.

        Common usage:

          self.combine_permuted(
            names=['set1a_set2a_set3a', 'set1b_set2b_set3b'],  
            p_columns=['p_subcol_normal', 'p_subcol_other'],
            p_column_combined='p_subcol_combined', out_column_suffix='_3')

        In this case, it first finds the existing tables that start with
        'set2a' and 'set3a', such as self.set2a_set1a_set3a_data and
        self.set3a_set1a_set2a_data. Then it makes the following
        averages:
          - 'p_subcol_normal_3' from column 'p_subcol_normal' of
          the three tables
          - 'p_subcol_other_3' from column 'p_subcol_other' of
          the three tables
          - 'p_subcol_combined_3: from the above two
        This procedure is repeated all elements of arg names.

        Returns a list of tables corresponding to arg names. Each table
        contains all the data of the corresponding input table,
        where the calculated averages are added as new columns. If arg
        update is True, the input tables are updated to include the 
        calculated columns. Otherwise new columns are returned
        and the original ones are left unchanged. 

        Arguments:
          - names: (list) names of the firstinput tables
          - p_columns: list of column names that contain p_values to be
          combined
          - p_column_combined: name of the column that normaly contains
          p_values from the normal and other random models
          - out_column_suffix: names of new columns are obtained by 
          adding this argument to the original column name
          - check_column: name of the column that should be the same 
          in all tables
          - update: flag indicating whether the input table is updated,
          or a new table is returned
          - verbose: flag indicating if a message is printed when a 
          table is not found

         Returns: list of tables corresponding to atg names, where each 
        element contains all data of the corresponding input table and the
        new columns, only if update is False.
        """

        res = []
        for nam in names:
            try:
                res_one = self.combine_permuted_one(
                    name=nam, other=None, p_columns=p_columns,
                    out_column_suffix=out_column_suffix, 
                    check_column=check_column, update=update, verbose=verbose)
            except ValueError:
                if not ignore: raise
            if not update:
                res.append(res_one)

        return res

    def combine_permuted_one(
            self, name, other=None, 
            p_columns=['p_subcol_normal', 'p_subcol_other'],
            p_column_combined='p_subcol_combined', out_column_suffix='_3', 
            check_column='distance', update=True, verbose=True):
        """
        Combines averages of p_values from collocalization data table 
        specifed by arg name and from other existing tables obtained 
        by permuting the order of particle sets comprising arg name.

        If arg other is None, all different combinations of the particle
        sets contained in arg name are checked. Otherwise, only the 
        tables specified by arg order are used. These tables have to 
        contain collocalization data for the same particle sets but in 
        different order.

        Implemented for combined tomos tables only.

        Implemented for 3-colocalizations only.

        Common usage:

          self.combine_permuted_one(
            name='set1_set2_set3', other=None, 
            p_columns=['p_subcol_normal', 'p_subcol_other'],
            p_column_combined='p_subcol_combined', out_column_suffix='_3')

        In this case, it first finds the existing tables that start with
        'set2' and 'set3', such as self.set2_set1_set3_data and
        self.set3_set1_set2_data. Then it makes the following
        averages:
          - 'p_subcol_normal_3' from column 'p_subcol_normal' of
          the three tables
          - 'p_subcol_other_3' from column 'p_subcol_other' of
          the three tables
          - 'p_subcol_combined_3: from the above two

        Returns a table that contains all the data of the (arg) name table,
        where the calculated averages are added as new columns. If arg
        update is True, the initial table specified by atg name is
        updated to the returned column. Otherwise a new column is returned
        and the original one is left unchanged. 

        Arguments:
          - name: name of the input table
          - other: list conatining names of other tables, or None to find
          the other tables based on the arg name 
          - p_columns: list of column names that contain p_values to be
          combined
          - p_column_combined: name of the column that normaly contains
          p_values from the normal and other random models
          - out_column_suffix: names of new columns are obtained by 
          adding this argument to the original column name
          - check_column: name of the column that should be the same 
          in all tables
          - update: flag indicating whether the input table is updated,
          or a new table is returned
          - verbose: flag indicating if a message is printed when a 
          table is not found

        Returns: table that contains all data of name and the new columns.
        """

        # get and check layers
        layers = col_func.get_layers(name=name)
        if len(layers) != 3:
            err_msg = (
                'P-values could not be combined because colocalization ' 
                + name + ' does not have exactly three layers')
            if verbose:
                print(err_msg)
            raise ValueError(err_msg)
        lay_1, lay_2, lay_3 = layers

        # find colocalizations to be combined
        tables = [self.get_data(name)[0]]
        if not update:
            tables[0] = tables[0].copy()
        if other is None:

            # try all possible combinations of layers
            try_name = self.make_table_names(layers=[lay_2, lay_1, lay_3])[0]
            if hasattr(self, try_name):
                tables.append(getattr(self, try_name))
            else:
                try_name = self.make_table_names(
                    layers=[lay_2, lay_3, lay_1])[0]
                if hasattr(self, try_name):
                    tables.append(getattr(self, try_name))
            try_name = self.make_table_names(layers=[lay_3, lay_1, lay_2])[0]
            if hasattr(self, try_name):
                tables.append(getattr(self, try_name))
            else:
                try_name = self.make_table_names(
                    layers=[lay_3, lay_2, lay_1])[0]
                if hasattr(self, try_name):
                    tables.append(getattr(self, try_name))

        else:
            tables = tables + [self.get_data(nam)[0] for nam in other]

        # check tables
        for ind in range(1,len(tables)):
            if not (tables[0][check_column] == tables[ind][check_column]).all():
                raise ValueError("Tables don't have the same row order")

        # combine p_values
        new_p_combined_col = p_column_combined + out_column_suffix
        for p_col in p_columns:
            new_p_col = p_col+out_column_suffix
            tables[0][new_p_col] = tables[0][p_col]
            for ind in range(1,len(tables)):
                tables[0][new_p_col] += tables[ind][p_col]
            tables[0][new_p_col] = tables[0][new_p_col] / float(len(tables))
            try:
                tables[0][new_p_combined_col] += tables[0][new_p_col]
            except KeyError:
                tables[0][new_p_combined_col] = tables[0][new_p_col]
        tables[0][new_p_combined_col] = (
            tables[0][new_p_combined_col] / float(len(p_columns)))

        return tables[0]

    def test_2_in_3_independence(
            self, names, join='inner', select_column='distance', value=None, 
            coloc_column='coloc'):
        """
        For all 3-colocalizations specified in arg names, tests whether 
        the corresponding 2-colocalizations that constitute these 
        3-colocalization are statistically independent.

        Uses test_2_in_3_independence_single()

        Specifically, for 3-colocalization set1_set2_set3, it calculates
        probabilities that a set1 particle is in 2-colocalizations
        set0_set1 and set0_set2. Based on these, the expected numbers of 
        set1 particles that belong and do not belong to set1_set2_set3 
        are calculated. The expecttion assumes that the 2- and 
        3-colocalizations are independent. The expected values are compared
        using chi-square with the real velues of set1_set2_set3.

        If args select_column and value are not None, returns only those
        rows where the selected column has the given value.

        Argument:
          - name: 3-colocalizstion name
          - join: the same as in pandas.concatenate(), controls how
          the tables corresponding to the individual elements of arg names
          are concatenated
          - select_column: name of the selection column
          - value: selection value
          - coloc_column: name of the column in the returned table that
          shows the individual 3-colocalization names

        Returns table containing expected, obtained and chi-square values
        """
        
        # test all colocalizations
        for nam in names:
            one = self.test_2_in_3_independence_single(name=nam)
            one[coloc_column] = nam
            try:
                result = pd.concat([result, one], join=join)
            except NameError:
                result = one

        # reorder columns
        old_cols = result.columns
        new_cols = [old_cols[0]] + [old_cols[-1]] + list(old_cols[1:-1])
        result = result[new_cols]

        # select rows
        if (select_column is not None) and (value is not None):
            result = result[result[select_column]==value]

        return result

    def test_2_in_3_independence_single(self, name):
        """
        Tests whether the 2-colocalizations that constitute the 
        3-colocalization specified by arg name are independent.

        Specifically, for 3-colocalization set1_set2_set3, it calculates
        probabilities that a set1 particle is in 2-colocalizations
        set0_set1 and set0_set2. Based on these, the expected numbers of 
        set1 particles that belong and do not belong to set1_set2_set3 
        are calculated. The expecttion assumes that the 2- and 
        3-colocalizations are independent. The expected values are compared
        using chi-square with the real velues of set1_set2_set3.

        Argument:
          - name: 3-colocalizstion name

        Returns table containing expected, obtained and chi-square values
        """

        # get particle names and data
        layers = col_func.get_layers(name)
        data = self.get_data(name)[0]
        data_01 = self.get_data(layers[0] + '_' + layers[1])[0]
        data_02 = self.get_data(layers[0] + '_' + layers[2])[0]
        n_lay0_total = 'n_'+layers[0]+'_total'

        # 3-coloc results
        res = data.loc[:,['distance', 'n_subcol']].copy()
        res['n_subcol F'] = (
            data.loc[:, 'n_'+layers[0]+'_total'] - data.loc[:, 'n_subcol'])

        # calculate expected 3-coloc results based on 2-colocs 
        res[layers[1]+' fract'] = (
            data_01.loc[:,'n_subcol'] / data_01.loc[:, n_lay0_total])
        res[layers[2]+' fract'] = (
            data_02.loc[:,'n_subcol'] / data_02.loc[:, n_lay0_total])
        res['expected T'] = (
            res[layers[1]+' fract'] * res[layers[2]+' fract'] 
            * data[n_lay0_total])
        res['expected F'] = data[n_lay0_total] - res['expected T']

        # stats
        for row in res.iterrows():
            chisq, pval = pyto.util.scipy_plus.chisquare_2(
                [row[1]['n_subcol'], row[1]['n_subcol F']],
                [row[1]['expected T'], row[1]['expected F']])
            res.loc[row[0], 'chisq'] = chisq
            res.loc[row[0], 'p-value'] = pval
        
        return res

    def enrichment_single(self, name, ref_name, subcol, layer_index):
        """
        Calculates whether particle type (set) in a 3-colocalization 
        (arg name) is enriched in respect to a reference particle set 
        in a reference 3-colocalization (arg ref_name). 

        For example, it can calculate wheteher a in 3-colocalization 
        preb-tether-a is enriched in respect to pstb in reference 
        3-colocalizations preb-tether-pstb.

        Uses chi-square test to compare the observed number of particles 
        that belong and not belong to a 3-colocalization with the expected 
        values. The expected values are calculated from the observed number 
        of refrerence particles in the reference 3-colocalization and the 
        total number of particles in the set that is analyzed and in the 
        reference set. 

        If arg subcol is True, observed and expected number of 
        3-colocalizations are statistically compared. For the total number 
        of analyzed and reference particles, layer 0 is used:

        Specifically:

          1) If subcol is False, name is preb_tether_n, ref_name is 
          preb_tether_pstb and layer_index is 2 the following quantities 
          are used:
              - number of particles n in preb_tether_n (observed value)
              - number of particles pstb in preb_tether_pstb
              - total number of n
              - total number of pstb

          2) If subcol is False, name is preb_tether_n, ref_name is 
          pre_tether_pst and layer_index is 1 the following quantities 
          are used:
              - number of particles preb in preb_tether_n (observed value)
              - number of particles pre in pre_tether_pst
              - total number of n
              - total number of pstb

          3) If subcol is True, name is preb_tether_a, ref_name is 
          pre_tether_pst:
              - number of 3-colocalizations in preb_tether_n (observed value)
              - number of 3-colocalization in preb_tether_pstb
              - total number of preb
              - total number of pre    

        Colocalization data tables (pandas.DataFrame) have to be defined before.
        The corresponding variable names have to be of the form name + '_data'.

        Arguments:
          - name: name of the analyzed 3-colocalization
          - ref_name: name of the reference 3-colocalization
          - subcol: flag indicating whether the number of 3-colocalizations
          is compared, or the number of particle in 3-colocalizations
          - layer_index: index that defines particle layer

        Returns table (pandas.DataFrame) that contains statistical observed,
        expected and statistical comparison values.
        """

        # get data
        data = self.get_data(name)[0]
        ref_data = self.get_data(ref_name)[0]

        # get names of columns that are red
        if subcol:

            layer_index = 0

            name_layer = col_func.get_layers(name)[layer_index]
            name_col = 'n_subcol'
            name_total_col = 'n_' + name_layer + '_total'

            ref_layer = col_func.get_layers(ref_name)[layer_index]
            ref_col = 'n_subcol'
            ref_total_col = 'n_' + ref_layer + '_total'

        else:

            name_layer = col_func.get_layers(name)[layer_index]
            name_col = 'n_' + name_layer + '_subcol'
            name_total_col = 'n_' + name_layer + '_total'

            ref_layer = col_func.get_layers(ref_name)[layer_index]
            ref_col = 'n_' + ref_layer + '_subcol'
            ref_total_col = 'n_' + ref_layer + '_total'

        # get names of columns to be made in the result table
        real_t_col = 'real_t'
        real_f_col = 'real_f'
        expected_t_col = 'exp_t'
        expected_f_col = 'exp_f'

        # put real and expected values in the result table
        result = data[['distance', 'n_subcol']].copy()
        result[real_t_col] = data[name_col]
        result[real_f_col] = data[name_total_col] - result[real_t_col]
        result[expected_t_col] = (
            ref_data[ref_col] * data[name_total_col] / ref_data[ref_total_col])
        result[expected_f_col] = data[name_total_col] - result[expected_t_col]

        # do stats
        for ind, row in result.iterrows():
            f1 = [np.array(row[real_t_col]), np.array(row[real_f_col])]
            f2 = [np.array(row[expected_t_col]), np.array(row[expected_f_col])]
            chi_val, p_value = pyto.util.scipy_plus.chisquare_2(f1, f2)
            result.loc[ind, 'chi2'] = chi_val
            result.loc[ind, 'p_value_chi2'] = p_value

        return result       
